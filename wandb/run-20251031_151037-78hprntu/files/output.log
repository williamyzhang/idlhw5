10/31/2025 15:10:38 - INFO - __main__ - ***** Training arguments *****
10/31/2025 15:10:38 - INFO - __main__ - Namespace(config='configs/ddpm.yaml', data_dir='./data', image_size=128, batch_size=32, num_workers=4, num_classes=100, run_name='exp-2-ddpm', output_dir='experiments', num_epochs=20, learning_rate=0.001, weight_decay=0.01, grad_clip=1.0, seed=42, mixed_precision='none', num_train_timesteps=1000, num_inference_steps=250, beta_start=0.0001, beta_end=0.02, beta_schedule='linear', variance_type='fixed_small', prediction_type='epsilon', clip_sample=True, clip_sample_range=1.0, unet_in_size=128, unet_in_ch=3, unet_ch=128, unet_ch_mult=[1, 2, 2, 4], unet_attn=[2, 3], unet_num_res_blocks=2, unet_dropout=0.1, latent_ddpm=False, use_cfg=False, cfg_guidance_scale=2.0, use_ddim=False, use_cifar10=True, ckpt=None, distributed=False, world_size=1, rank=0, local_rank=0, device='cuda', total_batch_size=32, max_train_steps=31260)
10/31/2025 15:10:38 - INFO - __main__ - ***** Running training *****
10/31/2025 15:10:38 - INFO - __main__ -   Num examples = 50000
10/31/2025 15:10:38 - INFO - __main__ -   Num Epochs = 20
10/31/2025 15:10:38 - INFO - __main__ -   Instantaneous batch size per device = 32
10/31/2025 15:10:38 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
10/31/2025 15:10:38 - INFO - __main__ -   Total optimization steps per epoch 1563
10/31/2025 15:10:38 - INFO - __main__ -   Total optimization steps = 31260
  0%|                                                                       | 0/31260 [00:00<?, ?it/s]10/31/2025 15:10:38 - INFO - __main__ - Epoch 1/20
  0%|                                                             | 1/31260 [00:00<5:28:38,  1.59it/s]10/31/2025 15:10:39 - INFO - __main__ - Epoch 1/20, Step 0/1563, Loss 0.995053768157959 (0.995053768157959)
  0%|▏                                                            | 101/31260 [00:11<58:23,  8.89it/s]10/31/2025 15:10:50 - INFO - __main__ - Epoch 1/20, Step 100/1563, Loss 1.0084409713745117 (1.0002804156577234)
  1%|▍                                                            | 201/31260 [00:23<58:10,  8.90it/s]10/31/2025 15:11:01 - INFO - __main__ - Epoch 1/20, Step 200/1563, Loss 0.9894942045211792 (0.9979953015621622)
  1%|▌                                                            | 301/31260 [00:34<57:54,  8.91it/s]10/31/2025 15:11:12 - INFO - __main__ - Epoch 1/20, Step 300/1563, Loss 0.9375728964805603 (0.9888729631306721)
  1%|▊                                                            | 401/31260 [00:45<57:46,  8.90it/s]10/31/2025 15:11:24 - INFO - __main__ - Epoch 1/20, Step 400/1563, Loss 1.0003067255020142 (0.9912409193795221)
  2%|▉                                                            | 501/31260 [00:56<57:34,  8.90it/s]10/31/2025 15:11:35 - INFO - __main__ - Epoch 1/20, Step 500/1563, Loss 0.9971122145652771 (0.9917095683054058)
  2%|█▏                                                           | 601/31260 [01:08<57:25,  8.90it/s]10/31/2025 15:11:46 - INFO - __main__ - Epoch 1/20, Step 600/1563, Loss 0.9884227514266968 (0.9913957359985186)
  2%|█▎                                                           | 701/31260 [01:19<57:11,  8.91it/s]10/31/2025 15:11:57 - INFO - __main__ - Epoch 1/20, Step 700/1563, Loss 0.9831644892692566 (0.990595700312273)
  3%|█▌                                                           | 801/31260 [01:30<56:58,  8.91it/s]10/31/2025 15:12:09 - INFO - __main__ - Epoch 1/20, Step 800/1563, Loss 0.965562105178833 (0.988791177112065)
  3%|█▋                                                           | 847/31260 [01:35<56:58,  8.90it/s]
